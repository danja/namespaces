# Recent Research on Using RDF, OWL and Ontologies with Large Language Models

Recent research has increasingly focused on bridging the gap between semantic web technologies and large language models (LLMs). The integration of Resource Description Framework (RDF), Web Ontology Language (OWL), and other ontological frameworks with LLMs represents a promising frontier for enhancing machine understanding of domain knowledge. This research spans approaches in training, fine-tuning, and in-context reasoning, with significant developments emerging in medical and other specialized fields.

## Ontology-Driven Training and Fine-Tuning Approaches

### Verbalization of Ontologies for Training Data

One significant challenge in using ontologies with LLMs has been the transformation of highly structured ontological knowledge into natural language that models can effectively process. Recent work has addressed this through ontology verbalization techniques that convert formal ontological statements into natural language text suitable for LLM consumption. Researchers have developed LLM-assisted verbalizers that create datasets by converting OWL statements from existing ontologies into natural text[1]. This approach aims to address the gap where ontologies do not fully cover domain knowledge, requiring tedious manual processes by domain experts to fill in missing concepts, relations, or axioms[1].

The verbalization process specifically targets the creation of text-OWL sentence pairs that can constitute training datasets for fine-tuning LLMs. By creating verbalized versions of ontological knowledge, researchers enable LLMs to potentially receive up-to-date and reliable domain knowledge in natural text and output structured graphs in standard ontology formats like OWL RDF/Turtle[1]. This represents a critical step in making semantic web technologies more accessible to modern deep learning approaches.

### Self-Training Frameworks for Ontology Alignment

Beyond simple verbalization, more sophisticated approaches involve self-training frameworks that explicitly align LLMs with ontological structures. The OntoTune framework exemplifies this approach, representing an ontology-driven self-training methodology designed to align LLMs with hierarchical conceptual knowledge[2]. This alignment addresses a fundamental limitation in current domain-specific LLMs, which typically develop through fine-tuning on domain-specific corpora but often fail to effectively organize domain knowledge, resulting in fragmented understanding[2].

OntoTune draws inspiration from human cognitive processes, particularly how humans connect concepts and organize knowledge through mind maps. The framework employs ontologies with hierarchical conceptual knowledge to reorganize an LLM's domain knowledge[2]. A key innovation in this approach is the use of in-context learning to identify whether the LLM has acquired specific concept-related ontology knowledge, selecting entries not yet mastered as the training set for further alignment[2].

## Ontology-Based In-Context Reasoning with LLMs

### Contextual Knowledge Representation and Reasoning

The integration of contextual reasoning capabilities with semantic web technologies represents another significant research direction. Researchers have worked on defining formally solid contextual models that are practically applicable using existing semantic web languages and tools[3]. These approaches aim to strengthen the links between theoretical and practical communities working on diverse aspects of representing and reasoning with contextual knowledge[3].

Recent work has introduced contextual extensions of web ontology languages, such as OWLC, based on two-dimensional description logic. This extension provides one language for the representation of context-dependent concepts, roles, or axioms, and a second language for representing contexts and their relations[3]. Such extensions enable more nuanced reasoning with contextual knowledge in RDF while committing to formally defined semantics of contextual description logic[3].

### Leveraging LLMs for Semantic Reasoning

Building on these contextual models, researchers have developed architectures for ontology-based semantic reasoning using LLMs. These architectures integrate knowledge graphs with language models to enhance reasoning capabilities, particularly in complex domains like healthcare[4]. By combining the natural language processing capabilities of LLMs with the structured knowledge representation of ontologies, these systems can extract meaningful information from diverse data sources and provide valuable insights[4].

The Reasoning-Based Thinking model exemplifies this approach, showcasing how ontological reasoning can support various healthcare applications, from early disease diagnosis to treatment recommendation systems[4]. These systems leverage different datasets and analytical techniques across diverse healthcare fields, demonstrating the versatility of ontology-based semantic reasoning with LLMs[4].

## Domain-Specific Applications in Healthcare

### Medical Ontologies for Enhanced Clinical Understanding

The healthcare domain has emerged as a particularly fertile ground for integrating ontologies with LLMs. Researchers have examined various methods and ontologies used in health services to develop robust frameworks for ontology-based semantic reasoning[4]. These frameworks address the complexity and multidimensionality of health data by applying advanced techniques to interpret and process clinical information[4].

Several studies exemplify this approach, including the development of dynamic methodologies for early diagnosis of Alzheimer's using ADNI and WHO data, leveraging ontological reasoning[4]. Other research has focused on major depressive disorder diagnosis using DSM parameters to assess depression severity, and diabetes diagnosis systems implementing Mamdani inference engines with fuzzy rules based on electronic health record data[4].

### Standardized Medical Ontologies as Knowledge Sources

The medical domain benefits particularly from the use of standardized ontologies like SNOMED CT to align LLMs with medical knowledge. Recent research has demonstrated that utilizing existing, long-term developed ontologies with LLMs significantly reduces data maintenance costs and offers improved generalization ability compared to approaches that rely on newly collected large-scale domain-specific corpora[2]. This suggests that ontologies can serve as efficient knowledge sources for domain adaptation of LLMs.

The OntoTune framework has shown state-of-the-art performance in both in-ontology tasks like hypernym discovery and out-of-ontology tasks like medical domain question answering when evaluated using the SNOMED CT medical ontology[2]. This demonstrates the potential of ontology-driven approaches to enhance LLM performance in specialized domains.

## Knowledge Graph Construction and Transformation

### LLM-Generated RDF Mappings

Another important research direction involves using LLMs to generate resource mapping language (RML) files in RDF turtle format as a step toward self-configuring RDF knowledge graph construction pipelines[5]. This approach leverages the capability of state-of-the-art LLMs to understand and generate data representation languages like RDF Turtle[5].

Recent case studies have explored mapping structured data sources like the Internet Movie Database (IMDB) in JSON format to target ontologies such as the DBpedia Ontology[5]. Researchers have defined and computed various scores to assess both the generated mapping files and the resulting graphs, finding promising potential for commercial LLMs even in zero-shot scenarios[5].

While LLMs have proven capable of generating RDF knowledge graphs from various input formats, challenges remain in terms of scalability and cost efficiency for large data transformations[5]. Traditional mapping and transformation approaches currently scale better for structured sources, but LLM-based approaches offer greater flexibility and automation potential[5].

## Conclusion

Recent research on using RDF, OWL, and other ontologies as input for LLMs demonstrates significant progress across multiple dimensions. Approaches range from verbalization techniques that transform ontological knowledge into natural language for training, to sophisticated self-training frameworks that align LLMs with ontological structures, to integrated architectures for ontology-based semantic reasoning. Domain-specific applications, particularly in healthcare, showcase the practical utility of these approaches.

The integration of semantic web technologies with large language models represents a promising direction for enhancing the knowledge representation and reasoning capabilities of AI systems. By combining the structured knowledge and formal semantics of ontologies with the natural language understanding capabilities of LLMs, researchers are developing more robust approaches to knowledge engineering, domain adaptation, and contextual reasoning. Future work will likely focus on addressing scalability challenges, improving the alignment between ontological knowledge and language model representations, and expanding applications to additional specialized domains.

Citations:
[1] https://ojs.aaai.org/index.php/AAAI-SS/article/view/31797
[2] https://openreview.net/forum?id=d7spHwemKX
[3] https://pdfs.semanticscholar.org/f124/4531bdc3b6e4ba67a557b2deedcacc309299.pdf
[4] https://www.thinkmind.org/articles/semapro_2024_1_20_30005.pdf
[5] https://openreview.net/pdf?id=ALgDqGCGdB
[6] http://xml.coverpages.org/W3CRec-RDF-OWL.html
[7] https://opendatascience.com/10-datasets-for-fine-tuning-large-language-models-llm/
[8] https://arxiv.org/html/2404.10317v1
[9] https://journals.sagepub.com/doi/full/10.1177/15705838241305067
[10] https://arxiv.org/html/2306.10723
[11] https://citeseerx.ist.psu.edu/document?doi=3b3dc1ca47e6611053068984c612ff745d20f8c9&repid=rep1&type=pdf
[12] https://openreview.net/forum?id=3x4vpeAclU
[13] https://www.w3.org/press-releases/2004/sws/
[14] https://www.aclweb.org/portal/content/ontology-structured-interfaces-semantics-4
[15] https://arxiv.org/html/2409.16220
[16] https://journals.sagepub.com/eprint/2TEMGXZDWDQWH4JFBJIW/full
[17] https://www.semanticscholar.org/paper/Fine-tuning-Large-Enterprise-Language-Models-via-Baldazzi-Bellomarini/1e7301ca09f604f56ee268c5ffaddcd7e537d46f
[18] https://openreview.net/forum?id=3x4vpeAclU
[19] https://www.semanticscholar.org/paper/LLMs4OL:-Large-Language-Models-for-Ontology-Giglou-D%E2%80%99Souza/78b1601c013769294a1927d43e50dfa81d6af75f
[20] https://www.scitepress.org/Papers/2024/128559/128559.pdf
[21] https://blog.gopenai.com/llm-ontology-prompting-for-knowledge-graph-extraction-efdcdd0db3a1?gi=1d12703d988b
[22] https://www.semanticscholar.org/paper/Fine-Tuning-Large-Language-Models-for-Ontology-A-of-Doumanas-Soularidis/d3bacd9a916f589e60bc2389aee8c632f3a2556d
[23] https://arxiv.org/html/2309.15074v2
[24] https://journals.sagepub.com/doi/10.1177/15705838241307486?int.sj-full-text.similar-articles.3
[25] https://dl.acm.org/doi/10.1016/j.jksuci.2021.08.018
[26] https://arxiv.org/html/2403.12776v1
[27] https://is.ijs.si/wp-content/uploads/2024/10/IS2024_-_SIKDD_2024_paper_28-1.pdf
[28] https://ai.wu.ac.at/~polleres/publications/delb-etal-2008.pdf
[29] https://www.w3.org/RDF/
[30] https://pmc.ncbi.nlm.nih.gov/articles/PMC11467320/
[31] https://arxiv.org/html/2404.14991v2
[32] https://www.linkedin.com/pulse/web3-why-llm-agents-better-option-than-rdf-owl-web-guado-zavaleta-yko4f
[33] https://arxiv.org/html/2408.13296v1
[34] https://arxiv.org/html/2409.10146v1

---
Answer from Perplexity: pplx.ai/share
